{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff4d6f4c-e2b1-4b53-afa3-62fdbd0f728d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Auteur : DIANI Mamoudou S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceb32bf-a9f1-46b6-a4d2-9251b9782cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import storage\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fb0cd4f-4212-4059-9d5d-99bbd713d38a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projet1-415505'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4372541-8d88-4cbd-a055-7265f4f1f1c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = !gcloud config list --format='value(core.account)' \n",
    "SERVICE_ACCOUNT = SERVICE_ACCOUNT[0]\n",
    "SERVICE_ACCOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2aba2232-d90e-40f8-8c4f-4f8f6d6a4f29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FRAMEWORK = 'tf'\n",
    "TASK = 'prediction'\n",
    "MODEL_TYPE = 'rnn'\n",
    "SCRIPT_PATH = './code/train.py'\n",
    "GCS_BUCKET=project[0]\n",
    "EXPERIMENT = '02a'\n",
    "SERIES = '02'\n",
    "GCS_BUCKET = PROJECT_ID\n",
    "DIR = f\"temp/{EXPERIMENT}\"\n",
    "URI = f\"gs://{GCS_BUCKET}/{SERIES}/{EXPERIMENT}\"\n",
    "REGION = 'northamerica-northeast1'\n",
    "EXPERIMENT_NAME = f'experiment-{SERIES}-{EXPERIMENT}-{FRAMEWORK}-{TASK}-{MODEL_TYPE}'\n",
    "BQ_PROJECT = project[0]\n",
    "BQ_DATASET = 'EURUSD'\n",
    "BQ_TABLE = 'EURUSD'\n",
    "BQ_TABLE_TRAIN = 'EURUSD_Train_prepped'\n",
    "BQ_TABLE_TEST = 'EURUSD_Test_prepped'\n",
    "VAR_TARGET = 'val_y_train'\n",
    "VAR_TARGET1 = 'val_y_test'\n",
    "TRAIN_COMPUTE = 'n1-standard-4'\n",
    "DEPLOY_COMPUTE = 'n1-standard-4'\n",
    "TRAIN_IMAGE = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-12.py310:latest'\n",
    "DEPLOY_IMAGE ='us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-12:latest' \n",
    "EPOCHS = 5\n",
    "BATCH_SIZE =10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f94b9e24-39ee-460d-8d10-868127e7c0d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(DIR):\n",
    "    os.makedirs(DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e16dffb-ea3d-42f2-a0ce-ff2e1ae63c21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gcs = storage.Client(project = PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9366df7-9d1b-46f0-a695-86fc4747318c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project = project[0], location = 'northamerica-northeast1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3f7ff1-d2c1-44c1-9a2c-82bd2f466cbb",
   "metadata": {},
   "source": [
    "**************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92462e8-4fac-4610-a4e8-0965f551e174",
   "metadata": {},
   "source": [
    "# Pipeline : Entraînement et mise en service continus\n",
    "Créer un travail de pipeline sur Vertex AI Pipelines qui réalise le flux de travail complet ci-dessus :\n",
    "\n",
    "1. Configuration initiale\n",
    "2. Entraînement du modèle\n",
    "3. Enregistrement du modèle\n",
    "4. Classement du modèle\n",
    "5. Validation du modèle (comparaison avec le modèle actuellement déployé)\n",
    "6. Mise à jour du point de terminaison\n",
    "7. Test du point de terminaison.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefdb39a-161b-44d4-ac2a-86ca9f8e26e2",
   "metadata": {},
   "source": [
    "**************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1a89d0-a100-4ddf-80dc-7667a5fa2b90",
   "metadata": {},
   "source": [
    "kfp ==> Kubeflow Pipelines (KFP) : Kubeflow est une plateforme open-source conçue pour faciliter le déploiement, la gestion et l'orchestration de workflows de machine learning (ML) sur Kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "001838c8-24cd-4db7-a521-e60b892ed0c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a40fe67-c5fb-46fc-89e2-5068d744a673",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfp.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38c4345-c570-4c79-831a-bd8fc53c38b0",
   "metadata": {},
   "source": [
    "## Stockage du code d'entraînement dans GCS\n",
    "\n",
    "Le fichier Python contenant le script d'entraînement doit être disponible localement lors de l'appel de la méthode from_local_script(). Toutefois, dans le cadre d'un job sur Vertex AI Pipelines, le fichier Python pourrait ne pas être accessible localement car le job s'exécute sur des ressources distantes. Pour résoudre ce problème, le fichier est stocké dans un emplacement spécifique sur Google Cloud Storage (GCS). Lors de l'exécution du job de pipeline, un système de fichiers FUSE (Filesystem in Userspace) est automatiquement mis en place pour monter le bucket GCS directement dans le conteneur du job à un emplacement déterminé.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0d5f78a-39a1-43f7-a855-03e7d07e0b17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket = gcs.lookup_bucket(GCS_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cfd128d-8527-4d5f-8293-63ad8b15e035",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./code/train.py'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCRIPT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2ccff0c-372a-44c4-981d-785249ea9ff9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train.py'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCRIPT_NAME = SCRIPT_PATH.split('/')[-1]\n",
    "SCRIPT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71afa669-4057-4a96-ae05-a64d501bc471",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob = bucket.blob(f'{SERIES}/{EXPERIMENT}/training/{SCRIPT_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e46d2f0-1f50-475a-91a7-64c32b3ff371",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob.upload_from_filename(SCRIPT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83813ab-bbbd-4937-9328-45c6fce2c6e1",
   "metadata": {},
   "source": [
    "***************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add689e1-14d4-4191-a78c-ce2d53f883a5",
   "metadata": {},
   "source": [
    "## Component: experiment_setup\n",
    "Définition d'un composant de pipeline Kubernetes pour la configuration d'expériences sur Vertex AI :\n",
    "\n",
    "1. Initialise un environnement avec une image de base Python et installe les packages nécessaires pour interagir avec les services Google Cloud.\n",
    "2. Définit une fonction pour configurer une expérience, qui prend plusieurs paramètres de projet, région, données BigQuery, etc.\n",
    "3. Utilise le client BigQuery pour interroger des échantillons de données de test.\n",
    "4. Configure un Tensorboard pour le suivi des expériences si disponible, sinon en crée un nouveau.\n",
    "5. Initialise une expérience dans Vertex AI avec le Tensorboard configuré.\n",
    "6. Prépare des arguments de commande pour l'exécution de modèles incluant des détails de l'expérience et des tables BigQuery.\n",
    "7. Reformate et prépare un échantillon de données de test pour l'entrée de modèle.\n",
    "8. Configure les URI des sources de données BigQuery pour un accès ultérieur.\n",
    "9. Crée des URIs pour l'accès direct aux expériences et aux exécutions d'expériences sur l'interface web de Vertex AI.\n",
    "10. Retourne les configurations, le nom de l'exécution, les détails du Tensorboard, et un échantillon de données pour l'utilisation ultérieure dans le pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c69786-c425-4ba5-a0be-c7857e8a6283",
   "metadata": {},
   "source": [
    "@kfp.dsl.component(\n",
    "    base_image = \"python:3.9\", \n",
    "    packages_to_install = [\"google-cloud-aiplatform\", \"google-cloud-pipeline-components\", \"google-cloud-bigquery\", \"pandas\", \"db-dtypes\"]\n",
    ")\n",
    "\n",
    "cette section de code définit une composante Kubeflow qui utilise une image de base Python 3.9 et installe certains packages Python spécifiques dans le conteneur pour permettre l'exécution de tâches spécifiques dans le pipeline Kubeflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55e914c3-de39-4943-99d3-f2f3be1ea5c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.component(\n",
    "    base_image = \"python:3.9\", \n",
    "    packages_to_install = [\"google-cloud-aiplatform\", \"google-cloud-pipeline-components\", \"google-cloud-bigquery\", \"pandas\", \"db-dtypes\"]\n",
    ")\n",
    "def experiment_setup(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    series: str,\n",
    "    experiment: str,\n",
    "    experiment_name: str,\n",
    "    cmdargs: list,\n",
    "    bq_project: str,\n",
    "    bq_dataset: str,\n",
    "    bq_table_train: str,\n",
    "    bq_table_test: str,\n",
    "    var_target: str,\n",
    "    var_target1: str,\n",
    "    bq_source: kfp.dsl.Output[artifact_types.BQTable]\n",
    ") -> NamedTuple('outputs', [\n",
    "    ('cmdargs', list), \n",
    "    ('run_name', str), \n",
    "    ('tensorboard', str), \n",
    "    ('timestamp', str),\n",
    "    ('experiment_uri', str),\n",
    "    ('experiment_run_uri', str),\n",
    "    ('sample', list)]):\n",
    "    \n",
    "   \n",
    "    from collections import namedtuple\n",
    "    result = namedtuple('outputs', ['cmdargs', 'run_name', 'tensorboard', 'timestamp', 'experiment_uri', 'experiment_run_uri', 'sample'])\n",
    "    \n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "    aiplatform.init(project = project, location = region)\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    bq = bigquery.Client(project = project)\n",
    "    \n",
    "    \n",
    "    tb = aiplatform.Tensorboard.list(filter=f\"labels.series={series}\")\n",
    "    if tb:\n",
    "        tb = tb[0]\n",
    "    else: \n",
    "        tb = aiplatform.Tensorboard.create(display_name = series, labels = {'series' : f'{series}'})\n",
    "    \n",
    "    \n",
    "    aiplatform.init(experiment = experiment_name, experiment_tensorboard = tb.resource_name)\n",
    "    \n",
    "    \n",
    "    from datetime import datetime\n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    RUN_NAME = f'run-{TIMESTAMP}'\n",
    "    CMDARGS = cmdargs + [\n",
    "        \"--project_id=\" + project,\n",
    "        \"--region=\" + region,\n",
    "        \"--series=\" + series,\n",
    "        \"--experiment=\" + experiment,\n",
    "        \"--experiment_name=\" + experiment_name,\n",
    "        \"--run_name=\" + RUN_NAME,\n",
    "        \"--bq_project=\" + bq_project,\n",
    "        \"--bq_dataset=\" + bq_dataset,\n",
    "        \"--bq_table_train=\" + bq_table_train,\n",
    "        \"--bq_table_test=\" + bq_table_test,\n",
    "        \"--var_target=\" + var_target,\n",
    "        \"--var_target1=\" + var_target1,\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    samples = bq.query(query = f\"\"\"\n",
    "        SELECT * \n",
    "        FROM `{bq_project}.{bq_dataset}.{bq_table_test}`\n",
    "        LIMIT 1\n",
    "    \"\"\").to_dataframe()\n",
    "    samples = samples.iloc[:, :-1]\n",
    "    samples = np.array(samples)\n",
    "    samples = np.reshape(samples, (samples.shape[0], samples.shape[1], 1))\n",
    "    samples = samples.tolist()\n",
    "    \n",
    "    \n",
    "    bq_source.uri1 = f'https://www.googleapis.com/bigquery/v2/projects/{bq_project}/datasets/{bq_dataset}/tables/{bq_table_train}'\n",
    "    bq_source.uri2 = f'https://www.googleapis.com/bigquery/v2/projects/{bq_project}/datasets/{bq_dataset}/tables/{bq_table_test}'\n",
    "    bq_source.metadata['projectId'] = bq_project\n",
    "    bq_source.metadata['datasetId'] = bq_dataset\n",
    "    bq_source.metadata['tableTrainId'] = bq_table_train\n",
    "    bq_source.metadata['tableTestId'] = bq_table_test\n",
    "    \n",
    "    \n",
    "    exp_uri = f'https://console.cloud.google.com/vertex-ai/locations/{region}/experiments/{experiment_name}?project={project}'\n",
    "    exp_run_uri = f'https://console.cloud.google.com/vertex-ai/locations/{region}/experiments/{experiment_name}/runs/{experiment_name}-{RUN_NAME}?project={project}'\n",
    "\n",
    "    \n",
    "    return result(CMDARGS, RUN_NAME, tb.resource_name, TIMESTAMP, exp_uri, exp_run_uri, samples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c263e1f-91f1-4d63-ba7f-8023c5fddd62",
   "metadata": {},
   "source": [
    "## Component: train_from_local_script\n",
    "\n",
    "Définition d'un composant de pipeline Kubernetes pour l'entraînement de modèles à partir de scripts locaux dans Vertex AI :\n",
    "\n",
    "1. Initialise un environnement avec une image de base Python et installe des packages nécessaires pour interagir avec Vertex AI et ses composants de pipeline.\n",
    "2. Définit une fonction pour configurer et exécuter un job d'entraînement personnalisé, qui utilise un script local pour l'entraînement.\n",
    "3. Utilise la fonction from_local_script de l'API Vertex AI pour créer un job personnalisé en spécifiant des paramètres comme le chemin du script, l'image du conteneur, les arguments du script, et la configuration du matériel.\n",
    "4. Configure des détails additionnels tels que les répertoires de sortie et les étiquettes pour l'organisation et le suivi.\n",
    "5. Exécute le job d'entraînement en spécifiant le compte de service et le Tensorboard associé pour le suivi des métriques.\n",
    "6. Génère des artefacts de sortie qui contiennent des informations sur l'état du job et les ressources GCP utilisées, accessibles via des URI dédiés pour la surveillance dans la console Google Cloud.\n",
    "7. Retourne l'état du job et une représentation JSON des ressources GCP utilisées pour une intégration facile avec d'autres services et outils Google Cloud.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5c5c7b-381b-4237-853c-5bae98e393c4",
   "metadata": {},
   "source": [
    "*********"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb82f142-2e94-44b8-8647-ba93300e39f9",
   "metadata": {},
   "source": [
    "* replica_count: C'est le nombre de réplicas (instances) à utiliser pour le job d'entraînement.\n",
    "* accelerator_count: C'est le nombre d'accélérateurs (par exemple, GPU) à attacher à chaque instance de la réplica pour accélérer le processus d'entraînement.\n",
    "* base_output_dir: C'est le répertoire de sortie de base où les artefacts de formation (modèles, checkpoints, etc.) seront sauvegardés dans Google Cloud Storage (GCS).\n",
    "* staging_bucket: C'est le bucket GCS utilisé pour le stockage temporaire des artefacts de formation pendant l'exécution du job d'entraînement.\n",
    "* labels: Ce sont des étiquettes (labels) facultatives qui peuvent être attachées au job d'entraînement pour organiser et catégoriser les exécutions. \n",
    "\n",
    "****************\n",
    "\n",
    "* Lors de l'exécution d'un composant, il est courant de voir non seulement le lien vers la tâche de composant lancée, mais également le lien vers les ressources cloud sous-jacentes, telles que les tâches de prédiction par lot Vertex ou les tâches Dataflow. Le proto gcp_resource est un paramètre spécial que vous pouvez utiliser dans votre composant pour permettre à Google Cloud Console d'offrir une vue personnalisée des journaux et de l'état de la ressource dans la console Vertex AI Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71a420b0-fe72-41ea-880a-3d5a181bb76c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.component(\n",
    "    base_image = \"python:3.9\",\n",
    "    packages_to_install = [\"google-cloud-aiplatform\", \"google-cloud-pipeline-components\"]\n",
    ")\n",
    "\n",
    "def train_from_local_script(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    series: str,\n",
    "    experiment: str,\n",
    "    experiment_name: str,\n",
    "    run_name: str,\n",
    "    cmdargs: list,\n",
    "    script: str,\n",
    "    train_image: str,\n",
    "    requirements: list,\n",
    "    train_compute: str,\n",
    "    timestamp: str,\n",
    "    bucket: str,\n",
    "    service_account: str,\n",
    "    tensorboard: str,\n",
    "    job_resources: kfp.dsl.Output[kfp.dsl.Artifact]\n",
    ") -> NamedTuple('outputs', [('state', str), ('gcp_resources', str)]):\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    result = namedtuple('outputs', ['state', 'gcp_resources'])\n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "    aiplatform.init(project = project, location = region)\n",
    "    \n",
    "    customJob = aiplatform.CustomJob.from_local_script(\n",
    "        display_name = f'{series}_{experiment}_{timestamp}',\n",
    "        script_path = f'/gcs/{bucket}/{series}/{experiment}/training/{script}',\n",
    "        container_uri = train_image,\n",
    "        args = cmdargs,\n",
    "        requirements = requirements,\n",
    "        replica_count = 1,\n",
    "        machine_type = train_compute,\n",
    "        accelerator_count = 0,\n",
    "        base_output_dir = f\"gs://{bucket}/{series}/{experiment}/models/{timestamp}\",\n",
    "        staging_bucket = f\"gs://{bucket}/{series}/{experiment}/models/{timestamp}\",\n",
    "        labels = {'series' : f'{series}', 'experiment' : f'{experiment}', 'experiment_name' : f'{experiment_name}', 'run_name' : f'{run_name}'}\n",
    "    )\n",
    "    \n",
    "    customJob.run(\n",
    "        service_account = service_account,\n",
    "        tensorboard = tensorboard\n",
    "    )\n",
    "    \n",
    "    job_resources.uri = f\"https://console.cloud.google.com/vertex-ai/locations/{region}/training/{customJob.resource_name.split('/')[-1]}/cpu\"\n",
    "    job_resources.metadata = dict(state = customJob.state.name, resource_name = customJob.resource_name, model = f\"gs://{bucket}/{series}/{experiment}/models/{timestamp}/model\")\n",
    "    \n",
    "    from google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResources\n",
    "    from google.protobuf.json_format import MessageToJson\n",
    "    customJob_resources = GcpResources()\n",
    "    cr = customJob_resources.resources.add()\n",
    "    cr.resource_type = 'CustomJob'\n",
    "    cr.resource_uri = f'https://{region}-aiplatform.googleapis.com/v1/{customJob.resource_name}'\n",
    "    gcp_resources = MessageToJson(customJob_resources)\n",
    "    \n",
    "    return result(customJob.state.name, gcp_resources)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbb1637-1a90-467d-8cb1-273bb81a3815",
   "metadata": {},
   "source": [
    "## Component: register_model\n",
    "\n",
    "Définition d'un composant de pipeline Kubernetes pour enregistrer un modèle d'apprentissage machine dans Vertex AI :\n",
    "\n",
    "1. Initialise un environnement avec une image de base Python et installe les packages nécessaires pour interagir avec Vertex AI.\n",
    "2. Définit une fonction pour enregistrer un modèle dans le registre de modèles de Vertex AI, utilisant divers paramètres de configuration comme le projet, la région, et le nom de l'expérience.\n",
    "3. Vérifie si le modèle spécifié existe déjà et si la version courante est déjà chargée ; si non, le modèle est chargé comme nouvelle version par défaut ou en tant que nouveau modèle.\n",
    "4. Utilise les métadonnées du job précédent pour configurer l'URI du modèle et les détails du conteneur de déploiement.\n",
    "5. Crée un lien vers le Tensorboard associé pour le suivi des métriques d'entraînement.\n",
    "6. Log les paramètres et les métadonnées du modèle dans l'exécution de l'expérience sur Vertex AI pour faciliter le suivi et l'audit.\n",
    "7. Met à jour l'état de l'exécution de l'expérience pour marquer la complétude de l'enregistrement du modèle.\n",
    "8. Configure et retourne un artefact représentant le modèle enregistré, incluant son URI et les métadonnées associées.\n",
    "9. Retourne l'ID de la version du modèle pour une utilisation ultérieure dans le pipeline ou d'autres opérations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a588377-3263-4e9d-ab5f-292eff20576d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.component(\n",
    "    base_image = \"python:3.9\",\n",
    "    packages_to_install = [\"google-cloud-aiplatform\", \"google-cloud-pipeline-components\"]\n",
    ")\n",
    "\n",
    "def register_model(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    series: str,\n",
    "    experiment: str,\n",
    "    experiment_name: str,\n",
    "    run_name: str,\n",
    "    deploy_image: str,\n",
    "    job_resources: kfp.dsl.Input[kfp.dsl.Artifact],\n",
    "    tensorboard: str,\n",
    "    vertex_model: kfp.dsl.Output[artifact_types.VertexModel]\n",
    ") -> NamedTuple('outputs', [('version_id', str)]):\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    result = namedtuple('outputs', ['version_id'])\n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "    aiplatform.init(project = project, location = region)\n",
    "    \n",
    "    modelmatch = aiplatform.Model.list(filter = f'display_name={series}_{experiment} AND labels.series={series} AND labels.experiment={experiment}')\n",
    "    \n",
    "    upload_model = True\n",
    "    if modelmatch:\n",
    "        print(\"Model Already in Registry:\")\n",
    "        if run_name in modelmatch[0].version_aliases:\n",
    "            print(\"This version already loaded, no action taken.\")\n",
    "            upload_model = False\n",
    "            model = aiplatform.Model(model_name = modelmatch[0].resource_name)\n",
    "        else:\n",
    "            print('Loading model as new default version.')\n",
    "            parent_model = modelmatch[0].resource_name\n",
    "    else:\n",
    "        print('This is a new model, creating in model registry')\n",
    "        parent_model = ''\n",
    "\n",
    "    if upload_model:\n",
    "        model = aiplatform.Model.upload(\n",
    "            display_name = f'{series}_{experiment}',\n",
    "            model_id = f'model_{series}_{experiment}',\n",
    "            parent_model =  parent_model,\n",
    "            serving_container_image_uri = deploy_image,\n",
    "            artifact_uri = job_resources.metadata['model'], \n",
    "            is_default_version = True,\n",
    "            version_aliases = [run_name],\n",
    "            version_description = run_name,\n",
    "            labels = {'series' : f'{series}', 'experiment' : f'{experiment}', 'experiment_name' : f'{experiment_name}', 'run_name' : f'{run_name}'}        \n",
    "        )\n",
    "    \n",
    "    customJob = aiplatform.CustomJob.get(resource_name = job_resources.metadata['resource_name'])\n",
    "    board_link = f\"https://{region}.tensorboard.googleusercontent.com/experiment/{tensorboard.replace('/', '+')}+experiments+{customJob.resource_name.split('/')[-1]}\"\n",
    "    \n",
    "    expRun = aiplatform.ExperimentRun(run_name = run_name, experiment = experiment_name)\n",
    "    expRun.log_params({\n",
    "        'model.uri': model.uri,\n",
    "        'model.display_name': model.display_name,\n",
    "        'model.name': model.name,\n",
    "        'model.resource_name': model.resource_name,\n",
    "        'model.version_id': model.version_id,\n",
    "        'model.versioned_resource_name': model.versioned_resource_name,\n",
    "        'customJobs.display_name': customJob.display_name,\n",
    "        'customJobs.resource_name': customJob.resource_name,\n",
    "        'customJobs.link': job_resources.uri,\n",
    "        'customJobs.tensorboard': board_link\n",
    "    })\n",
    "    \n",
    "    expRun.update_state(state = aiplatform.gapic.Execution.State.COMPLETE)\n",
    "    \n",
    "    vertex_model.uri = f'https://{region}-aiplatform.googleapis.com/v1/{model.versioned_resource_name}'\n",
    "    vertex_model.metadata['model_resource_name'] = model.versioned_resource_name\n",
    "    \n",
    "    return result(model.version_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd2b596-cd09-4480-b14b-09ac8b9e4d22",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Component: endpoint_model\n",
    "\n",
    "Composant Kubernetes pour gérer les endpoints de modèles dans Vertex AI :\n",
    "\n",
    "1. Initialise un client Vertex AI spécifique au projet et à la région donnés.\n",
    "2. Liste et vérifie l'existence d'un endpoint avec un nom d'affichage spécifique et des labels associés.\n",
    "3. Si l'endpoint existe déjà, il est récupéré ; sinon, un nouvel endpoint est créé avec le nom d'affichage et les labels appropriés.\n",
    "4. Enregistre l'URI et les métadonnées de l'endpoint existant ou nouvellement créé dans un artefact de sortie pour un accès ultérieur.\n",
    "5. Parcourt les modèles associés à l'endpoint et, pour chaque modèle, vérifie si celui-ci reçoit 100% du trafic (c'est-à-dire, le modèle actuellement déployé).\n",
    "6. Enregistre l'URI et les métadonnées du modèle actuellement déployé dans un autre artefact de sortie pour référence et utilisation futures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c02f1b3c-36ff-4d41-b99a-f46fd3d31eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.component(\n",
    "    base_image = \"python:3.9\",\n",
    "    packages_to_install = [\"google-cloud-aiplatform\", \"google-cloud-pipeline-components\", 'pandas']\n",
    ")\n",
    "\n",
    "def endpoint_model(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    series: str,\n",
    "    vertex_endpoint: kfp.dsl.Output[artifact_types.VertexEndpoint],\n",
    "    vertex_model: kfp.dsl.Output[artifact_types.VertexModel]\n",
    "):\n",
    "    \n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "    aiplatform.init(project = project, location = region)\n",
    "    \n",
    "    endpoints = aiplatform.Endpoint.list(filter = f\"labels.series={series} AND display_name={series}\")\n",
    "    if endpoints:\n",
    "        endpoint = endpoints[0]\n",
    "        print(f\"Endpoint Exists: {endpoints[0].resource_name}\")\n",
    "    else:\n",
    "        endpoint = aiplatform.Endpoint.create(\n",
    "            display_name = f\"{series}\",\n",
    "            labels = {'series' : f\"{series}\"}    \n",
    "        )\n",
    "        print(f\"Endpoint Created: {endpoint.resource_name}\")\n",
    "\n",
    "    vertex_endpoint.uri = f'https://{region}-aiplatform.googleapis.com/v1/{endpoint.resource_name}'\n",
    "    vertex_endpoint.metadata['endpoint_resource_name'] = endpoint.resource_name\n",
    "    \n",
    "\n",
    "    for model in endpoint.list_models():\n",
    "        if endpoint.traffic_split[model.id] == 100:\n",
    "            vertex_model.uri = f'https://{region}-aiplatform.googleapis.com/v1/{model.model}@{model.model_version_id}'\n",
    "            vertex_model.metadata['model_resource_name'] = model.model+'@'+model.model_version_id\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b237b7f-fea8-45fb-942e-3eade2849495",
   "metadata": {},
   "source": [
    "## Component: rank_model\n",
    "\n",
    "Composant Kubernetes pour classer les modèles d'apprentissage machine dans Vertex AI :\n",
    "\n",
    "1. Initialise un client Vertex AI spécifique au projet et à la région donnés.\n",
    "2. Liste toutes les expériences associées à une série spécifique et récupère leurs données dans un DataFrame.\n",
    "3. Définit une fonction pour classer les modèles en fonction de leur performance (perte de test) à la fois dans l'ensemble de la série et dans chaque expérience.\n",
    "4. Ajoute une étiquette 'Status' pour identifier le nouveau modèle et le modèle actuellement déployé.\n",
    "5. Si le modèle actuellement déployé n'est pas trouvé, ajoute manuellement ses données pour éviter des erreurs dans les classements.\n",
    "6. Réorganise les colonnes pour mettre 'Status' en premier, facilitant la visualisation.\n",
    "7. Calcule les rangs pour le nouveau modèle et le modèle actuellement déployé pour fournir des métriques comparatives.\n",
    "8. Exporte le tableau complet des rangs au format Markdown dans un fichier spécifié.\n",
    "9. Retourne les rangs expérimentaux et de série pour le nouveau modèle ainsi que le rang de série pour le modèle actuellement déployé.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "82c8b58d-423d-4fa0-a271-7554911f3724",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.component(\n",
    "    base_image = \"python:3.9\",\n",
    "    packages_to_install = [\"google-cloud-aiplatform\", \"google-cloud-pipeline-components\", 'pandas']\n",
    ")\n",
    "def rank_model(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    series: str,\n",
    "    current_model: kfp.dsl.Input[artifact_types.VertexModel], \n",
    "    new_model: kfp.dsl.Input[artifact_types.VertexModel],\n",
    "    ranks_table: kfp.dsl.Output[kfp.dsl.Markdown]\n",
    ") -> NamedTuple('outputs', [('new_model_experiment_rank', float), ('new_model_series_rank', float), ('current_model_series_rank', float)]):\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    result = namedtuple('outputs', ['new_model_experiment_rank', 'new_model_series_rank', 'current_model_series_rank'])\n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "    aiplatform.init(project = project, location = region)\n",
    "\n",
    "    import pandas as pd\n",
    "    \n",
    "    experiments = aiplatform.Experiment.list()\n",
    "    experiments = [e for e in experiments if e.name.split('-')[0:2] == ['experiment', series]]\n",
    "    results = []\n",
    "    for experiment in experiments:\n",
    "            results.append(experiment.get_data_frame())\n",
    "            print(experiment.name)\n",
    "    results = pd.concat(results)\n",
    "    \n",
    "    def ranker(metric = 'metric.test_loss'):\n",
    "        ranks = results[['experiment_name', 'run_name', 'param.model.display_name', 'param.model.version_id', metric]].copy().reset_index(drop = True)\n",
    "        ranks = ranks[ranks['param.model.display_name'].notnull()]\n",
    "        ranks['series_rank'] = ranks[metric].rank(method = 'dense', ascending = True)\n",
    "        ranks['experiment_rank'] = ranks.groupby('experiment_name')[metric].rank(method = 'dense', ascending = True)\n",
    "        return ranks.sort_values(by = ['series_rank', 'experiment_rank'])\n",
    "    ranks = ranker('metric.test_loss') \n",
    "    \n",
    "    new_model = aiplatform.Model(model_name = new_model.metadata['model_resource_name'])\n",
    "    ranks.loc[(ranks['param.model.display_name'] == new_model.display_name) & (ranks['param.model.version_id'] == new_model.version_id), 'Status'] = 'New Model'\n",
    "    \n",
    "    try :\n",
    "        current_model = aiplatform.Model(model_name = current_model.metadata['model_resource_name'])\n",
    "        ranks.loc[(ranks['param.model.display_name'] == current_model.display_name) & (ranks['param.model.version_id'] == current_model.version_id), 'Status'] = 'On Endpoint'\n",
    "        \n",
    "    except KeyError:\n",
    "        data = {\n",
    "    'experiment_name': [None],\n",
    "    'run_name': [None],\n",
    "    'param.model.display_name': ['current_model'],\n",
    "    'param.model.version_id': ['1'],\n",
    "    'metric.test_loss': [1000],\n",
    "    'series_rank': [1000],\n",
    "    'experiment_rank': [1000],\n",
    "    'Status': 'On Endpoint'\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        ranks= pd.concat([ranks, df], ignore_index=True)\n",
    "        \n",
    "    col = ranks.pop('Status')\n",
    "    ranks.insert(0, col.name, col)\n",
    "    \n",
    "    new_rank = ranks.loc[(ranks['param.model.display_name'] == new_model.display_name) & (ranks['param.model.version_id'] == new_model.version_id)]\n",
    "    current_rank = ranks.loc[(ranks['param.model.display_name'] == \"current_model\") & (ranks['param.model.version_id'] == '1')]\n",
    "    \n",
    "    with open(ranks_table.path, 'w') as f:\n",
    "        f.write(ranks.to_markdown(index = False))\n",
    "    \n",
    "    return result(new_rank['experiment_rank'].iloc[0], new_rank['series_rank'].iloc[0], current_rank['series_rank'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ae3278-ed9b-4a48-b409-3a478354610a",
   "metadata": {},
   "source": [
    "## Component: deploy_model\n",
    "\n",
    "Composant Kubernetes pour déployer un modèle de machine learning sur un endpoint dans Vertex AI \n",
    "1. Initialise un client Vertex AI pour le projet et la région spécifiés.\n",
    "2. Récupère les ressources existantes de l'endpoint et du modèle à partir de leurs métadonnées.\n",
    "3. Déploie le modèle sur l'endpoint spécifié avec une allocation de trafic de 100%, et configure le type de machine et les compteurs de réplicas.\n",
    "4. Vérifie les modèles déployés sur l'endpoint pour ajuster la répartition du trafic ou retirer les anciens modèles.\n",
    "5. Détermine si le modèle spécifié a été correctement déployé et récupère le pourcentage de trafic qui lui est attribué.\n",
    "6. Retourne le nombre total de modèles déployés, un booléen indiquant si le nouveau modèle a été déployé, et le pourcentage de trafic alloué au nouveau modèle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fb12dfd3-365c-4e6f-8f8c-527ae4ac615b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.component(\n",
    "    base_image = \"python:3.9\",\n",
    "    packages_to_install = [\"google-cloud-aiplatform\", \"google-cloud-pipeline-components\"]\n",
    ")\n",
    "def deploy_model(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    deploy_compute: str,\n",
    "    vertex_endpoint: kfp.dsl.Input[artifact_types.VertexEndpoint], \n",
    "    vertex_model: kfp.dsl.Input[artifact_types.VertexModel]\n",
    ") -> NamedTuple('outputs', [('count_of_deployed', int), ('new_model_deployed', bool), ('traffic_new_model', int)]):\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    result = namedtuple('outputs', ['count_of_deployed', 'new_model_deployed', 'traffic_new_model'])\n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "    aiplatform.init(project = project, location = region)\n",
    "    \n",
    "    endpoint = aiplatform.Endpoint(endpoint_name = vertex_endpoint.metadata['endpoint_resource_name'])\n",
    "    model = aiplatform.Model(model_name = vertex_model.metadata['model_resource_name'])\n",
    "\n",
    "    endpoint.deploy(\n",
    "        model = model,\n",
    "        deployed_model_display_name = model.display_name,\n",
    "        traffic_percentage = 100,\n",
    "        machine_type = deploy_compute,\n",
    "        min_replica_count = 1,\n",
    "        max_replica_count = 1\n",
    "    )\n",
    "\n",
    "    deployed = False\n",
    "    traffic = 0\n",
    "    for deployed_model in endpoint.list_models():\n",
    "        if deployed_model.id not in endpoint.traffic_split:\n",
    "            endpoint.undeploy(deployed_model_id = deployed_model.id)\n",
    "        elif f'{deployed_model.model}@{deployed_model.model_version_id}' == vertex_model.metadata['model_resource_name']:\n",
    "            deployed = True\n",
    "            traffic = endpoint.traffic_split[deployed_model.id]\n",
    "\n",
    "    info = endpoint.to_dict()\n",
    "    \n",
    "    return result(\n",
    "        len(info['deployedModels']),\n",
    "        deployed,\n",
    "        traffic\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a660d6a0-5338-443b-85e6-e86a79b99018",
   "metadata": {},
   "source": [
    "## Component: prediction_test\n",
    "\n",
    "Composant Kubernetes pour effectuer des tests de prédiction sur un modèle déployé dans Vertex AI :\n",
    "\n",
    "1. Initialise un client Vertex AI spécifié par le projet et la région.\n",
    "2. Récupère l'endpoint de Vertex AI à partir de ses métadonnées pour accéder au modèle déployé.\n",
    "3. Effectue une prédiction en utilisant un échantillon de données fourni comme entrée.\n",
    "4. Retourne les résultats de la prédiction ainsi que le nom de ressource du modèle utilisé pour la prédiction, incluant l'identifiant de version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "10553460-2e10-4069-a73e-f40854826dbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.component(\n",
    "    base_image = \"python:3.9\",\n",
    "    packages_to_install = [\"google-cloud-aiplatform\", \"google-cloud-pipeline-components\"]\n",
    ")\n",
    "def prediction_test(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    sample: list,\n",
    "    vertex_endpoint: kfp.dsl.Input[artifact_types.VertexEndpoint]\n",
    ") -> NamedTuple('outputs', [('predictions', list), ('model_resource_name', str)]):\n",
    "\n",
    "    from collections import namedtuple\n",
    "    result = namedtuple('outputs', ['predictions', 'model_resource_name'])\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "    aiplatform.init(project = project, location = region) \n",
    "    \n",
    "    endpoint = aiplatform.Endpoint(endpoint_name = vertex_endpoint.metadata['endpoint_resource_name'])\n",
    "    \n",
    "    prediction = endpoint.predict(instances = [sample])\n",
    "    \n",
    "    return result(\n",
    "        predictions = prediction.predictions[0],\n",
    "        model_resource_name = prediction.model_resource_name + '@' + prediction.model_version_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c5f1cc-b487-4640-b12b-cd8edad6faf0",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "Définition d'un pipeline complet de développement dans Vertex AI pour la série et l'expérience spécifiées :\n",
    "Ce pipeline intègre toutes les étapes nécessaires depuis la configuration initiale jusqu'au déploiement du modèle, y compris l'entraînement, l'enregistrement et les tests de prédiction.\n",
    "\n",
    "Étapes du pipeline :\n",
    "1. Configuration de l'expérience : Initialise les paramètres de l'expérience, configure les ressources de BigQuery et prépare les arguments de commande.\n",
    "2. Entraînement du modèle : Exécute un script local pour l'entraînement du modèle sur les données spécifiées, en utilisant les ressources de calcul définies.\n",
    "3. Vérification de la complétion de l'entraînement : Contrôle si l'entraînement a réussi avant de procéder.\n",
    "4. Modèle de point d'entrée avant la mise à jour : Capture l'état actuel du point d'entrée pour la comparaison de modèles.\n",
    "5. Enregistrement du modèle : Enregistre le modèle entraîné dans le registre de modèles Vertex AI, prêt pour le déploiement.\n",
    "6. Classement du modèle : Compare les performances du nouveau modèle avec celles du modèle actuellement déployé pour décider de la mise à jour.\n",
    "7. Déploiement du modèle : Si le nouveau modèle est jugé supérieur, il est déployé au point d'entrée avec le trafic redirigé vers lui.\n",
    "8. Modèle de point d'entrée après la mise à jour : Capture le nouvel état du point d'entrée après le déploiement du modèle.\n",
    "9. Test de prédiction : Effectue un test de prédiction pour vérifier les performances du modèle déployé.\n",
    "\n",
    "Chaque étape du pipeline est configurée pour utiliser des limites de CPU spécifiques et des options de mise en cache pour optimiser les performances et les coûts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "77ce7531-733f-4bd1-a3ff-063c6e299303",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name = f'series-{SERIES}-{EXPERIMENT}-pipeline',\n",
    "    description = 'Full development pipeline.'\n",
    ")\n",
    "\n",
    "\n",
    "def retrain_pipeline(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    series: str,\n",
    "    experiment: str,\n",
    "    experiment_name: str,\n",
    "    bq_project: str,\n",
    "    bq_dataset: str,\n",
    "    bq_table_train: str,\n",
    "    bq_table_test: str,\n",
    "    var_target: str,\n",
    "    var_target1: str,\n",
    "    cmdargs: list,\n",
    "    script: str,\n",
    "    train_image: str,\n",
    "    requirements: list,\n",
    "    train_compute: str,\n",
    "    deploy_image: str,\n",
    "    deploy_compute: str,\n",
    "    bucket: str,\n",
    "    service_account: str\n",
    "):\n",
    "    \n",
    "    from google_cloud_pipeline_components.types import artifact_types\n",
    "    \n",
    "    setup = experiment_setup(\n",
    "        project = project,\n",
    "        region = region,\n",
    "        series = series,\n",
    "        experiment = experiment,\n",
    "        experiment_name = experiment_name,\n",
    "        cmdargs = cmdargs,\n",
    "        bq_project = bq_project,\n",
    "        bq_dataset = bq_dataset,\n",
    "        bq_table_train = bq_table_train,\n",
    "        bq_table_test = bq_table_test,\n",
    "        var_target = var_target,\n",
    "        var_target1 = var_target1).set_display_name('Setup').set_cpu_limit('2').set_caching_options(True)\n",
    "    \n",
    "    trainer = train_from_local_script(\n",
    "        project = project,\n",
    "        region = region,\n",
    "        series = series,\n",
    "        experiment = experiment,\n",
    "        experiment_name = experiment_name,\n",
    "        run_name = setup.outputs['run_name'],\n",
    "        cmdargs = setup.outputs['cmdargs'],\n",
    "        script = script,\n",
    "        train_image = train_image,\n",
    "        requirements = requirements,\n",
    "        train_compute = train_compute,\n",
    "        timestamp = setup.outputs['timestamp'],\n",
    "        bucket = bucket,\n",
    "        service_account = service_account,\n",
    "        tensorboard = setup.outputs['tensorboard']\n",
    "    ).set_display_name('Train With Script').set_cpu_limit('2').set_caching_options(True)\n",
    "    \n",
    "\n",
    "    with kfp.dsl.If(\n",
    "        trainer.outputs['state'] == 'JOB_STATE_SUCCEEDED',\n",
    "        name = 'Check Training Completion'\n",
    "    ):\n",
    "    \n",
    "        endpoint_before = endpoint_model(\n",
    "            project = project,\n",
    "            region = region,\n",
    "            series = series\n",
    "        ).set_display_name('Endpoint: Before Update').set_cpu_limit('2').set_caching_options(False)\n",
    "        \n",
    "        model = register_model(\n",
    "            project = project,\n",
    "            region = region,\n",
    "            series = series,\n",
    "            experiment = experiment,\n",
    "            experiment_name = experiment_name,\n",
    "            run_name = setup.outputs['run_name'],\n",
    "            deploy_image = deploy_image,\n",
    "            job_resources = trainer.outputs['job_resources'],\n",
    "            tensorboard = setup.outputs['tensorboard']\n",
    "        ).set_display_name('Register Model').set_cpu_limit('2').set_caching_options(False)\n",
    "    \n",
    "        rank = rank_model(\n",
    "            project = project,\n",
    "            region = region,\n",
    "            series = series,\n",
    "            new_model = model.outputs['vertex_model'],\n",
    "            current_model = endpoint_before.outputs['vertex_model']\n",
    "        ).set_display_name('Rank Model').set_cpu_limit('2').set_caching_options(False)\n",
    "        \n",
    "        with kfp.dsl.If(\n",
    "            rank.outputs['new_model_series_rank'] < rank.outputs['current_model_series_rank'],\n",
    "            name = 'Bless New Model'\n",
    "        ):\n",
    "            \n",
    "            deploy = deploy_model(\n",
    "                project = project,\n",
    "                region = region,\n",
    "                deploy_compute = deploy_compute,\n",
    "                vertex_endpoint = endpoint_before.outputs['vertex_endpoint'],\n",
    "                vertex_model = model.outputs['vertex_model']\n",
    "            ).set_display_name('Deploy Model').set_cpu_limit('2').set_caching_options(False)\n",
    "            \n",
    "            endpoint_after = endpoint_model(\n",
    "                project = project,\n",
    "                region = region,\n",
    "                series = series\n",
    "            ).set_display_name('Endpoint: After Update').set_cpu_limit('2').set_caching_options(False).after(deploy)\n",
    "            \n",
    "            \n",
    "            with kfp.dsl.If(\n",
    "                deploy.outputs['new_model_deployed'] == True,\n",
    "                name = 'Check The Model Deployment'\n",
    "            ):\n",
    "    \n",
    "                prediction = prediction_test(\n",
    "                    project = project,\n",
    "                    region = region,\n",
    "                    sample = setup.outputs['sample'],\n",
    "                    vertex_endpoint = endpoint_after.outputs['vertex_endpoint'],\n",
    "                ).set_display_name('Prediction').set_cpu_limit('2').set_caching_options(False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef88b7b-7c60-49a1-aa1e-90b7872cebab",
   "metadata": {},
   "source": [
    "## Compile Pipeline\n",
    "\n",
    "Compiler le pipeline en utilisant kfp.compiler.Compiler()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "66da023c-cc26-40ec-8a13-67657d0989f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func = retrain_pipeline,\n",
    "    package_path = f\"{DIR}/{EXPERIMENT}.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e435786-9b7b-4d6c-991d-73418eef93a9",
   "metadata": {},
   "source": [
    "## Definie Pipeline Job\n",
    "\n",
    "- Initialiser les parametres d'entree \n",
    "- creer un pipeline job avec aiplatform.PipelineJob()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c113658c-3615-48a8-b6eb-799920dad441",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REQUIREMENTS = ['tensorflow_io', f'google-cloud-aiplatform==1.34.0', 'db-dtypes', f\"protobuf>=3.19.5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f8e9108b-587c-4f22-950b-8ebaa7de843f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CMDARGS = [\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--batch_size=\" + str(BATCH_SIZE)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bfea480d-8b95-491e-ad99-e954a9fd7f5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameter_values = {\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"region\": REGION,\n",
    "    \"series\": SERIES,\n",
    "    \"experiment\": EXPERIMENT,\n",
    "    \"experiment_name\": EXPERIMENT_NAME,\n",
    "    \"bq_project\": BQ_PROJECT,\n",
    "    \"bq_dataset\": BQ_DATASET,\n",
    "    \"bq_table_train\": BQ_TABLE_TRAIN,\n",
    "    \"bq_table_test\": BQ_TABLE_TEST,\n",
    "    \"var_target\": VAR_TARGET,\n",
    "    \"var_target1\": VAR_TARGET1,\n",
    "    \"cmdargs\": CMDARGS,\n",
    "    \"script\": SCRIPT_NAME,\n",
    "    \"train_image\": TRAIN_IMAGE,\n",
    "    \"requirements\": REQUIREMENTS,\n",
    "    \"train_compute\": TRAIN_COMPUTE,\n",
    "    \"deploy_image\": DEPLOY_IMAGE,\n",
    "    \"deploy_compute\": DEPLOY_COMPUTE,\n",
    "    \"bucket\": GCS_BUCKET,\n",
    "    \"service_account\": SERVICE_ACCOUNT\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "97103681-c7e9-4023-8d6e-7bd4de5b5e35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name = f\"{SERIES}_{EXPERIMENT}\",\n",
    "    template_path = f\"{DIR}/{EXPERIMENT}.json\",\n",
    "    parameter_values = parameter_values,\n",
    "    pipeline_root = f\"{URI}/pipeline_root\",\n",
    "    enable_caching = None, # True (enabled), False (disable), None (defer to component level caching) \n",
    "    labels = {'series': SERIES, 'experiment': EXPERIMENT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7c2fa8-fbc8-42dc-8192-ee64928a121c",
   "metadata": {},
   "source": [
    "## Soumettre le travail de pipeline\n",
    "\n",
    "Soumettre le travail de pipeline pour exécution avec aiplatform.PipelineJob.submit()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1b76f26f-b747-4301-9ddf-ad0f5be37de3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/3515132901/locations/northamerica-northeast1/pipelineJobs/series-02-02a-pipeline-20240413062807\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/3515132901/locations/northamerica-northeast1/pipelineJobs/series-02-02a-pipeline-20240413062807')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/northamerica-northeast1/pipelines/runs/series-02-02a-pipeline-20240413062807?project=3515132901\n"
     ]
    }
   ],
   "source": [
    "response = pipeline_job.submit(\n",
    "    service_account = SERVICE_ACCOUNT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5e908605-0a63-4e3f-95f3-6f7a4a883247",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dashboard can be viewed here:\n",
      "https://console.cloud.google.com/vertex-ai/locations/northamerica-northeast1/pipelines/runs/series-02-02a-pipeline-20240413062807?project=3515132901\n"
     ]
    }
   ],
   "source": [
    "print(f'The Dashboard can be viewed here:\\n{pipeline_job._dashboard_uri()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6ecfaf-e363-4eb0-b0b5-f91556f0ebd2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Attendre le Job Pipeline\n",
    "\n",
    "mettre en attente l'exécution jusqu'à ce que le travail du pipeline soit terminé avec aiplatform.PipelineJob.wait()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "332c00d5-3a87-4c2c-bd5f-a6021dae6239",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/3515132901/locations/northamerica-northeast1/pipelineJobs/series-02-02a-pipeline-20240413062807 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/3515132901/locations/northamerica-northeast1/pipelineJobs/series-02-02a-pipeline-20240413062807 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/3515132901/locations/northamerica-northeast1/pipelineJobs/series-02-02a-pipeline-20240413062807 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/3515132901/locations/northamerica-northeast1/pipelineJobs/series-02-02a-pipeline-20240413062807 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/3515132901/locations/northamerica-northeast1/pipelineJobs/series-02-02a-pipeline-20240413062807 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/3515132901/locations/northamerica-northeast1/pipelineJobs/series-02-02a-pipeline-20240413062807 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/3515132901/locations/northamerica-northeast1/pipelineJobs/series-02-02a-pipeline-20240413062807 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/3515132901/locations/northamerica-northeast1/pipelineJobs/series-02-02a-pipeline-20240413062807\n"
     ]
    }
   ],
   "source": [
    "pipeline_job.wait()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m116",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m116"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
