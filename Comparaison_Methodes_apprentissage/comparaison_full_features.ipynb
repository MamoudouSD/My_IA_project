{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# DIANI Mamoudou Sékou\n",
    "#\n",
    "# Janvier 2022\n",
    "#==========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recupération des données situées dans \"C:/Users/diani/Desktop/Apprntissage_auto/DataPrepa1.csv\"\n",
    "# lecture du CSV et affichage des 5 premieres lignes\n",
    "Pollution_data = pd.read_csv('C:/Users/diani/Desktop/Apprntissage_auto/DataPrepa1.csv', sep = ';', index_col=\"id\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "Pollution_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation des donnees avec la normalisation z-score\n",
    "features = [\"tLengthOfScreenName\", \"tLengthOfDescriptionInUserProfile\", \"tLongevityOfTheAccount\", \"tNumerOfFollowings\", \"tNumberOfFollowers\", \"tSeriesOfNumberOfFollowings\", \"Following.followers\", \"tNumberOfTweets\", \"tNumberOfTweetsByDays\", \"Tweets.theLongevityOfTheAccount\", \"TweetURL.tNumberOfTweets\", \"tNbMoyenUrlByTweet\", \"UsernameInTweet.tNumberOfTweets\", \"TimeMoyenBet2ConseTweet\", \"ValueOfTimeMaxBet2ConseTweet\"]\n",
    "std_scale = preprocessing.StandardScaler().fit(Pollution_data[features])\n",
    "normalized_data = std_scale.transform(Pollution_data[features])\n",
    "#print(normalized_data)\n",
    "labels = np.array(Pollution_data['Status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separation des donnees en données d'entrenement et des données de teste\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(normalized_data, labels, test_size = 0.2, random_state = 42)\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette fonction permet de déterminer les metriques de performance \n",
    "# True Positive Rate(TPR), False Positive Rate(FPR) et l'aire sous la courbe ROC (Receiver Operating Characteristic)\n",
    "def TPR_FPR(test_labels, predicted_labels):\n",
    "    cnf_matrix = confusion_matrix(test_labels, predicted_labels)\n",
    "    #print (cnf_matrix)\n",
    "    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)\n",
    "    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "    TP = np.diag(cnf_matrix)\n",
    "    TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "    FP = FP.astype(float)\n",
    "    FN = FN.astype(float)\n",
    "    TP = TP.astype(float)\n",
    "    TN = TN.astype(float)\n",
    "\n",
    "    TPR = TP/(TP+FN)\n",
    "    FPR = FP/(FP+TN)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(test_labels, predicted_labels, pos_label=1)\n",
    "    aire = metrics.auc(fpr, tpr)\n",
    "    return (TPR, FPR, aire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_TPR=\"True Positive Rate: \"\n",
    "str_FPR=\"False Positive Rate: \"\n",
    "str_Aire=\"l'aire sous la courbe ROC: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arbre de décision: entrainement de l'arbre et prédiction sur les données de test\n",
    "# Et determination de la F-mesure, du FPR, TPR et de l'aire sous la courbe\n",
    "ad_classifier = DecisionTreeClassifier(random_state=42)\n",
    "d_tree1 = ad_classifier.fit(train_features,train_labels)\n",
    "predicted_labels = d_tree1.predict(test_features)\n",
    "\n",
    "tpr, fpr, ad_auc  =TPR_FPR(test_labels, predicted_labels)\n",
    "Val_ad_tpr=tpr[1]\n",
    "Val_ad_fpr=fpr[1]\n",
    "f1 = f1_score(test_labels, predicted_labels, average='binary', pos_label=1)\n",
    "print (str_TPR, Val_ad_tpr)\n",
    "print (str_FPR, Val_ad_fpr)\n",
    "print (str_Aire, ad_auc)\n",
    "print(\"F1-score of decision tree: \", f1)\n",
    "# True Positive Rate:  0.9237988826815643\n",
    "# False Positive Rate:  0.09019607843137255\n",
    "# l'aire sous la courbe ROC:  0.9168014021250959\n",
    "# F1-score of decision tree: 0.923386196113469"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forrest: entrainement et prédiction sur les données de test\n",
    "# Et determination de la F-mesure, du FPR, TPR et de l'aire sous la courbe\n",
    "rnd_forest  = RandomForestClassifier()\n",
    "rnd_forest.fit(train_features,train_labels)\n",
    "predicted_labels = rnd_forest.predict(test_features)\n",
    "\n",
    "tpr, fpr, rf_auc  =TPR_FPR(test_labels, predicted_labels)\n",
    "Val_rf_tpr=tpr[1]\n",
    "Val_rf_fpr=fpr[1]\n",
    "f1 = f1_score(test_labels, predicted_labels, average='binary', pos_label=1)\n",
    "print (str_TPR, Val_rf_tpr)\n",
    "print (str_FPR, Val_rf_fpr)\n",
    "print (str_Aire, rf_auc)\n",
    "print(\"F1-score of random forrest: \", f1)\n",
    "# True Positive Rate:  0.9557541899441341\n",
    "# False Positive Rate:  0.058823529411764705\n",
    "# l'aire sous la courbe ROC:  0.9484653302661846\n",
    "# F1-score of random forrest: 0.9536438600401158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging: entrainement et prédiction sur les données de test\n",
    "# Et determination de la F-mesure, du FPR, TPR et de l'aire sous la courbe\n",
    "bagging = BaggingClassifier()\n",
    "bagging_classifier = bagging.fit(train_features,train_labels)\n",
    "predicted_labels = bagging_classifier.predict(test_features)\n",
    "\n",
    "tpr, fpr, bag_auc  =TPR_FPR(test_labels, predicted_labels)\n",
    "Val_bag_tpr=tpr[1]\n",
    "Val_bag_fpr=fpr[1]\n",
    "print (str_TPR,Val_bag_tpr)\n",
    "print (str_FPR,Val_bag_fpr)\n",
    "print (str_Aire,bag_auc)\n",
    "f1 = f1_score(test_labels, predicted_labels, average='binary', pos_label=1)\n",
    "print(\"F1-score of bagging: \", f1)\n",
    "# True Positive Rate:  0.9448044692737431\n",
    "# False Positive Rate:  0.05908496732026144\n",
    "# l'aire sous la courbe ROC:  0.9428597509767407\n",
    "# F1-score of bagging: 0.9475810977663038"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost: entrainement et prédiction sur les données de test\n",
    "# Et determination de la F-mesure, du FPR, TPR et de l'aire sous la courbe\n",
    "adaB = AdaBoostClassifier()\n",
    "adaB_classifier = adaB.fit(train_features,train_labels)\n",
    "predicted_labels = adaB_classifier.predict(test_features)\n",
    "\n",
    "tpr, fpr, adaB_auc  =TPR_FPR(test_labels, predicted_labels)\n",
    "Val_adaB_tpr=tpr[1]\n",
    "Val_adaB_fpr=fpr[1]\n",
    "print (str_TPR,Val_adaB_tpr)\n",
    "print (str_FPR,Val_adaB_fpr)\n",
    "print (str_Aire,adaB_auc)\n",
    "f1 = f1_score(test_labels, predicted_labels, average='binary', pos_label=1)\n",
    "print(\"F1-score of adaBoost: \", f1)\n",
    "# True Positive Rate:  0.9452513966480447\n",
    "# False Positive Rate:  0.06745098039215686\n",
    "# l'aire sous la courbe ROC:  0.9389002081279441\n",
    "# F1-score of adaBoost: 0.9438803971884413"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification avec Naive Bayes: entrainement et prédiction sur les données de test\n",
    "# Et determination de la F-mesure, du FPR, TPR et de l'aire sous la courbe\n",
    "bayes_class=GaussianNB()\n",
    "bayes_classifier = bayes_class.fit(train_features,train_labels)\n",
    "predicted_labels = bayes_classifier.predict(test_features)\n",
    "\n",
    "tpr, fpr, nb_auc  =TPR_FPR(test_labels, predicted_labels)\n",
    "Val_nb_tpr=tpr[1]\n",
    "Val_nb_fpr=fpr[1]\n",
    "print (str_TPR,Val_nb_tpr)\n",
    "print (str_FPR,Val_nb_fpr)\n",
    "print (str_Aire,nb_auc)\n",
    "f1 = f1_score(test_labels, predicted_labels, average='binary', pos_label=1)\n",
    "print(\"F1-score of Naive bayes: \", f1)\n",
    "# True Positive Rate:  0.6538547486033519\n",
    "# False Positive Rate:  0.08418300653594771\n",
    "# l'aire sous la courbe ROC:  0.7848358710337021\n",
    "# F1-score of Naive bayes: 0.7577366308429367"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
