{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# DIANI Mamoudou Sékou\n",
    "#\n",
    "# Fevrier 2022\n",
    "#===========================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recupération des données situées dans \"C:/Users/diani/Desktop/Apprntissage_auto/DataPrepa1.csv\"\n",
    "# lecture du CSV et affichage des 5 premieres lignes\n",
    "Pollution_data = pd.read_csv('C:/Users/diani/Desktop/Apprntissage_auto/DataPrepa1.csv', sep = ';', index_col=\"id\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "Pollution_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation des donnees avec la normalisation z-score\n",
    "features = [\"tLengthOfScreenName\", \"tLengthOfDescriptionInUserProfile\", \"tLongevityOfTheAccount\", \"tNumerOfFollowings\", \"tNumberOfFollowers\", \"tSeriesOfNumberOfFollowings\", \"Following.followers\", \"tNumberOfTweets\", \"tNumberOfTweetsByDays\", \"Tweets.theLongevityOfTheAccount\", \"TweetURL.tNumberOfTweets\", \"tNbMoyenUrlByTweet\", \"UsernameInTweet.tNumberOfTweets\", \"TimeMoyenBet2ConseTweet\", \"ValueOfTimeMaxBet2ConseTweet\"]\n",
    "std_scale = preprocessing.StandardScaler().fit(Pollution_data[features])\n",
    "normalized_data = std_scale.transform(Pollution_data[features])\n",
    "#print(normalized_data)\n",
    "labels = np.array(Pollution_data['Status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recherche des 7 meilleures features avec le gain d'information\n",
    "mutuals = mutual_info_classif(normalized_data, labels)\n",
    "\n",
    "# Recupération des donnees et tries du plus imortant au moins important\n",
    "mutual_info = []\n",
    "for i in range (len(mutuals)):\n",
    "    mutual_info.append([features[i], mutuals[i]])\n",
    "res = sorted(mutual_info, key = lambda x : x[1], reverse=True)\n",
    "\n",
    "\n",
    "#Affichage des 7 meilleurs features et de leurs scores chi2 \n",
    "liste_newFeatures=[]\n",
    "for i in range (7):\n",
    "    x=res[i]\n",
    "    liste_newFeatures.append(x[0])\n",
    "print(liste_newFeatures)\n",
    "position=[]\n",
    "for i in range (len(liste_newFeatures)):\n",
    "    for j in range (len(features)):\n",
    "        if liste_newFeatures[i] == features[j]:\n",
    "            position.append(j)\n",
    "print (position)\n",
    "\n",
    "### Elements du dataset à supprimer\n",
    "l=[]\n",
    "a=0\n",
    "for i in range (len(features)):\n",
    "    if i in position:\n",
    "        a=a+1\n",
    "    else:\n",
    "        l.append(i)\n",
    "#print (l)\n",
    "\n",
    "\n",
    "\n",
    "#Determination de la nouvelle liste de données\n",
    "newList=[]\n",
    "for i in range(len(normalized_data)):\n",
    "    myArray = np.array(normalized_data[i])\n",
    "    modifiedArray = np.delete(myArray, l)\n",
    "    newList.append(modifiedArray)\n",
    "normalized_data=newList\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separation des donnees en données d'entrenement et des données de test\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(normalized_data, labels, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette fonction permet de déterminer les metriques de performance \n",
    "# True Positive Rate(TPR), False Positive Rate(FPR) et l'aire sous la courbe ROC (Receiver Operating Characteristic)\n",
    "def TPR_FPR(test_labels, predicted_labels):\n",
    "    cnf_matrix = confusion_matrix(test_labels, predicted_labels)\n",
    "    #print (cnf_matrix)\n",
    "    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)\n",
    "    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "    TP = np.diag(cnf_matrix)\n",
    "    TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "    FP = FP.astype(float)\n",
    "    FN = FN.astype(float)\n",
    "    TP = TP.astype(float)\n",
    "    TN = TN.astype(float)\n",
    "\n",
    "    TPR = TP/(TP+FN)\n",
    "    FPR = FP/(FP+TN)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(test_labels, predicted_labels, pos_label=1)\n",
    "    aire = metrics.auc(fpr, tpr)\n",
    "    return (TPR, FPR, aire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_TPR=\"True Positive Rate: \"\n",
    "str_FPR=\"False Positive Rate: \"\n",
    "str_Aire=\"l'aire sous la courbe ROC: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arbre de décision: entrainement de l'arbre et prédiction sur les données de test\n",
    "# Et determination de la F-mesure, du FPR, TPR et de l'aire sous la courbe\n",
    "ad_classifier = DecisionTreeClassifier(random_state=42)\n",
    "d_tree1 = ad_classifier.fit(train_features,train_labels)\n",
    "predicted_labels = d_tree1.predict(test_features)\n",
    "\n",
    "tpr, fpr, ad_auc  =TPR_FPR(test_labels, predicted_labels)\n",
    "Val_ad_tpr=tpr[1]\n",
    "Val_ad_fpr=fpr[1]\n",
    "f1 = f1_score(test_labels, predicted_labels, average='binary', pos_label=1)\n",
    "print (str_TPR, Val_ad_tpr)\n",
    "print (str_FPR, Val_ad_fpr)\n",
    "print (str_Aire, ad_auc)\n",
    "f1 = f1_score(test_labels, predicted_labels, average='binary', pos_label=1)\n",
    "print(\"F1-score of decision tree: \", f1)\n",
    "# True Positive Rate:  0.9068156424581005\n",
    "# False Positive Rate:  0.10431372549019607\n",
    "# l'aire sous la courbe ROC:  0.9012509584839523\n",
    "# F1-score of decision tree: 0.9086430810568741"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forrest: entrainement et prédiction sur les données de test\n",
    "# Et determination de la F-mesure, du FPR, TPR et de l'aire sous la courbe\n",
    "rnd_forest  = RandomForestClassifier()\n",
    "rnd_forest.fit(train_features,train_labels)\n",
    "predicted_labels = rnd_forest.predict(test_features)\n",
    "\n",
    "tpr, fpr, rf_auc  =TPR_FPR(test_labels, predicted_labels)\n",
    "Val_rf_tpr=tpr[1]\n",
    "Val_rf_fpr=fpr[1]\n",
    "f1 = f1_score(test_labels, predicted_labels, average='binary', pos_label=1)\n",
    "print (str_TPR, Val_rf_tpr)\n",
    "print (str_FPR, Val_rf_fpr)\n",
    "print (str_Aire, rf_auc)\n",
    "f1 = f1_score(test_labels, predicted_labels, average='binary', pos_label=1)\n",
    "print(\"F1-score of random forrest: \", f1)\n",
    "# True Positive Rate:  0.9494972067039106\n",
    "# False Positive Rate:  0.07581699346405228\n",
    "# l'aire sous la courbe ROC:  0.9368401066199291\n",
    "# F1-score of random forrest: 0.9427273734352498"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging: entrainement et prédiction sur les données de test\n",
    "# Et determination de la F-mesure, du FPR, TPR et de l'aire sous la courbe\n",
    "bagging = BaggingClassifier()\n",
    "bagging_classifier = bagging.fit(train_features,train_labels)\n",
    "predicted_labels = bagging_classifier.predict(test_features)\n",
    "\n",
    "tpr, fpr, bag_auc  =TPR_FPR(test_labels, predicted_labels)\n",
    "Val_bag_tpr=tpr[1]\n",
    "Val_bag_fpr=fpr[1]\n",
    "print (str_TPR,Val_bag_tpr)\n",
    "print (str_FPR,Val_bag_fpr)\n",
    "print (str_Aire,bag_auc)\n",
    "f1 = f1_score(test_labels, predicted_labels, average='binary', pos_label=1)\n",
    "print(\"F1-score of bagging: \", f1)\n",
    "# True Positive Rate:  0.9356424581005587\n",
    "# False Positive Rate:  0.07111111111111111\n",
    "# l'aire sous la courbe ROC:  0.9322656734947238\n",
    "# F1-score of bagging: 0.9343603482920295"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost: entrainement et prédiction sur les données de test\n",
    "# Et determination de la F-mesure, du FPR, TPR et de l'aire sous la courbe\n",
    "adaB = AdaBoostClassifier()\n",
    "adaB_classifier = adaB.fit(train_features,train_labels)\n",
    "predicted_labels = adaB_classifier.predict(test_features)\n",
    "\n",
    "tpr, fpr, adaB_auc  =TPR_FPR(test_labels, predicted_labels)\n",
    "Val_adaB_tpr=tpr[1]\n",
    "Val_adaB_fpr=fpr[1]\n",
    "print (str_TPR,Val_adaB_tpr)\n",
    "print (str_FPR,Val_adaB_fpr)\n",
    "print (str_Aire,adaB_auc)\n",
    "f1 = f1_score(test_labels, predicted_labels, average='binary', pos_label=1)\n",
    "print(\"F1-score of adaBoost: \", f1)\n",
    "# True Positive Rate:  0.9325139664804469\n",
    "# False Positive Rate:  0.07477124183006537\n",
    "# l'aire sous la courbe ROC:  0.9288713623251909\n",
    "# F1-score of adaBoost: 0.9341840161182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification avec Naive Bayes: entrainement et prédiction sur les données de test\n",
    "# Et determination de la F-mesure, du FPR, TPR et de l'aire sous la courbe\n",
    "bayes_class=GaussianNB()\n",
    "bayes_classifier = bayes_class.fit(train_features,train_labels)\n",
    "predicted_labels = bayes_classifier.predict(test_features)\n",
    "\n",
    "tpr, fpr, nb_auc  =TPR_FPR(test_labels, predicted_labels)\n",
    "Val_nb_tpr=tpr[1]\n",
    "Val_nb_fpr=fpr[1]\n",
    "print (str_TPR,Val_nb_tpr)\n",
    "print (str_FPR,Val_nb_fpr)\n",
    "print (str_Aire,nb_auc)\n",
    "f1 = f1_score(test_labels, predicted_labels, average='binary', pos_label=1)\n",
    "print(\"F1-score of Naive bayes: \", f1)\n",
    "# True Positive Rate:  0.7707262569832403\n",
    "# False Positive Rate:  0.13333333333333333\n",
    "# l'aire sous la courbe ROC:  0.8186964618249535\n",
    "# F1-score of Naive bayes: 0.8178800094854162"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
